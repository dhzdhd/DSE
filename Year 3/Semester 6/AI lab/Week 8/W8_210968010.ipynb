{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b5c2c2-689d-4666-aeda-6f03fa223a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa1af34-10fb-4385-9f9c-50ee4a2122ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd49e1c3-2503-4ac6-967b-d8dcaa012990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "        # Number of evaluation iterations\n",
    "        evaluation_iterations = 1\n",
    "        # Initialize a value function for each state as zero\n",
    "        V = np.zeros(16)\n",
    "        # Repeat until change in value is below the threshold\n",
    "        for i in range(int(max_iterations)):\n",
    "                # Initialize a change of value function as zero\n",
    "                delta = 0\n",
    "                # Iterate though each state\n",
    "                for state in range(16):\n",
    "                       # Initial a new value of current state\n",
    "                       v = 0\n",
    "                       # Try all possible actions which can be taken from this state\n",
    "                       for action, action_probability in enumerate(policy[state]):\n",
    "                             # Check how good next state will be\n",
    "                             for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                                  # Calculate the expected value\n",
    "                                  v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
    "\n",
    "                       # Calculate the absolute change of value function\n",
    "                       delta = max(delta, np.abs(V[state] - v))\n",
    "                       # Update value function\n",
    "                       V[state] = v\n",
    "                evaluation_iterations += 1\n",
    "\n",
    "                # Terminate if value change is insignificant\n",
    "                if delta < theta:\n",
    "                        print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
    "                        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9f87cb-a2d4-4b6c-bb28-897d9c18231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "        action_values = np.zeros(4)\n",
    "        for action in range(4):\n",
    "                for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                        action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caeddc22-6a64-4adb-b0a4-0f347822a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "        # Start with a random policy\n",
    "        #num states x num actions / num actions\n",
    "        policy = np.ones([16, 4]) / 4\n",
    "        # Initialize counter of evaluated policies\n",
    "        evaluated_policies = 1\n",
    "        # Repeat until convergence or critical number of iterations reached\n",
    "        for i in range(int(max_iterations)):\n",
    "                stable_policy = True\n",
    "                # Evaluate current policy\n",
    "                V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "                # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "                for state in range(16):\n",
    "                        # Choose the best action in a current state under current policy\n",
    "                        current_action = np.argmax(policy[state])\n",
    "                        # Look one step ahead and evaluate if current action is optimal\n",
    "                        # We will try every possible action in a current state\n",
    "                        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                        # Select a better action\n",
    "                        best_action = np.argmax(action_value)\n",
    "                        # If action didn't change\n",
    "                        if current_action != best_action:\n",
    "                                stable_policy = True\n",
    "                                # Greedy policy update\n",
    "                                policy[state] = np.eye(4)[best_action]\n",
    "                evaluated_policies += 1\n",
    "                # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "                if stable_policy:\n",
    "                        print(f'Evaluated {evaluated_policies} policies.')\n",
    "                        return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09bfb969-c48e-4e62-a51c-86830c61f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "        # Initialize state-value function with zeros for each environment state\n",
    "        V = np.zeros(16)\n",
    "        for i in range(int(max_iterations)):\n",
    "                # Early stopping condition\n",
    "                delta = 0\n",
    "                # Update each state\n",
    "                for state in range(16):\n",
    "                        # Do a one-step lookahead to calculate state-action values\n",
    "                        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                        # Select best action to perform based on the highest state-action value\n",
    "                        best_action_value = np.max(action_value)\n",
    "                        # Calculate change in value\n",
    "                        delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "                        # Update the value function for current state\n",
    "                        V[state] = best_action_value\n",
    "                        # Check if we can stop\n",
    "                if delta < theta:\n",
    "                        print(f'Value-iteration converged at iteration#{i}.')\n",
    "                        break\n",
    "\n",
    "        # Create a deterministic policy using the optimal value function\n",
    "        policy = np.zeros([16, 4])\n",
    "        for state in range(16):\n",
    "                # One step lookahead to find the best action for this state\n",
    "                action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                # Select best action based on the highest state-action value\n",
    "                best_action = np.argmax(action_value)\n",
    "                # Update the policy to perform a better action at a current state\n",
    "                policy[state, best_action] = 1.0\n",
    "        return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb6de89-6b8f-4772-8230-f252eb5effa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Value iteration algorithm to solve MDP.\n",
    "\n",
    "def value_iteration(env, discount=1e-1, theta=1e-9, max_iter=1e4):\n",
    "\n",
    "    # initalized state-value function with zeros for each env state\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(int(max_iter)):\n",
    "        # early stopping condition\n",
    "        delta = 0\n",
    "        # update each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            # Do a one-step lookahead to calculate state-action values\n",
    "            action_value = one_step_lookahead(env, state, V, discount)\n",
    "            # select best action to perform based on the highest state-action values\n",
    "            best_action_value = np.max(action_value)\n",
    "            # calculate change in value\n",
    "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "            # update the value function for current state\n",
    "            V[state] = best_action_value\n",
    "\n",
    "        # check if we can stop\n",
    "        if delta < theta:\n",
    "            print(f'\\nValue iteration converged at iteration #{i+1:,}')\n",
    "            break\n",
    "\n",
    "    # create deterministic policy using the optimal value function\n",
    "    policy = np.zeros(shape=[env.observation_space.n, 4])\n",
    "\n",
    "    for state in range(env.observation_space.n):\n",
    "        # one step lookahead to find the best action for this state\n",
    "        action_value = one_step_lookahead(env, state, V, discount)\n",
    "        #select the best action based on the highest state-action value\n",
    "        best_action = np.argmax(action_value)\n",
    "        # update the policy to perform a better action at a current state\n",
    "        policy[state, best_action] = 1.0\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06374064-b57f-47b3-ac56-4a8cb3620f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 66 iterations.\n",
      "Evaluated 2 policies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programming\\DSE\\.conda\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration :: number of wins over 1000 episodes = 776\n",
      "Policy Iteration :: average reward over 1000 episodes = 0.776 \n",
      "\n",
      "\n",
      "\n",
      "Value iteration converged at iteration #8\n",
      "Value Iteration :: number of wins over 1000 episodes = 477\n",
      "Value Iteration :: average reward over 1000 episodes = 0.477 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "        wins = 0\n",
    "        total_reward = 0\n",
    "        for episode in range(n_episodes):\n",
    "                terminated = False\n",
    "                state,info = environment.reset()\n",
    "                while not terminated:\n",
    "                        # Select best action to perform in a current state\n",
    "                        action = np.argmax(policy[state])\n",
    "                        # Perform an action an observe how environment acted in response\n",
    "                        next_state, reward, terminated, _, info = environment.step(action)\n",
    "                        # Summarize total reward\n",
    "                        total_reward += reward\n",
    "                        # Update current state\n",
    "                        state = next_state\n",
    "                        # Calculate number of wins over episodes\n",
    "                        if terminated and reward == 1.0:\n",
    "                                wins += 1\n",
    "        average_reward = total_reward / n_episodes\n",
    "        return wins, total_reward, average_reward\n",
    "\n",
    "# Number of episodes to play\n",
    "n_episodes = 1000\n",
    "# Functions to find best policy\n",
    "solvers = [('Policy Iteration', policy_iteration),\n",
    "           ('Value Iteration', value_iteration)]\n",
    "for iteration_name, iteration_func in solvers:\n",
    "        # Load a Frozen Lake environment\n",
    "        environment = gym.make('FrozenLake-v1')\n",
    "        # Search for an optimal policy using policy iteration\n",
    "        policy, V = iteration_func(environment.env)\n",
    "        # Apply best policy to the real environment\n",
    "        wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "        print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "        print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d73de8-d28c-4199-9b7b-7ff3fd42404f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
